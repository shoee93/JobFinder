#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø´Ø±ÙˆØ¹: JobFinder (Ù†Ø³Ø®Ø© Ø¹Ø±Ø¨ÙŠØ© Ù…ÙØ´ÙŽØ±Ù‘ÙŽØ­Ø©)
--------------------------------------
Ù‡Ø°Ø§ Ø§Ù„Ø³ÙƒØ±Ø¨Øª ÙŠØ¨Ø­Ø« Ø¹Ù† ÙˆØ¸Ø§Ø¦Ù Ù…Ù† Ø®Ù„Ø§ØµØ§Øª RSS Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ØªÙˆØ¸ÙŠÙØŒ
Ø«Ù… ÙŠØ­Ù„Ù‘Ù„ Ù†ØµÙˆØµ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª ÙˆÙŠÙ‚Ø§Ø±Ù†Ù‡Ø§ Ù…Ø¹ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© ØªØ­Ø¯Ø¯Ù‡Ø§ Ø£Ù†Øª.
Ø¨Ø¹Ø¯Ù‡Ø§ ÙŠØ±ØªÙ‘Ø¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­Ø³Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚ØŒ ÙŠØ­ÙØ¸Ù‡Ø§ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª SQLite ÙˆÙ…Ù„Ù CSVØŒ
ÙˆÙŠÙˆÙ„Ù‘Ø¯ Ø®Ø·Ø§Ø¨Ø§Øª ØªÙ‚Ø¯ÙŠÙ… (Anschreiben) Ø¬Ø§Ù‡Ø²Ø© Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ Ù‚Ø§Ù„Ø¨.
"""

# -----------------------------
# 1) Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©
# -----------------------------
import os
import re
import csv
import hashlib
import sqlite3
import time
from datetime import datetime
from typing import List, Dict, Any, Optional

import feedparser                 # Ù„Ù‚Ø±Ø§Ø¡Ø© Ø®Ù„Ø§ØµØ§Øª RSS
import requests                   # Ù„Ø¬Ù„Ø¨ ØµÙØ­Ø§Øª Ø§Ù„ÙˆÙŠØ¨
from bs4 import BeautifulSoup     # Ù„ØªØ­Ù„ÙŠÙ„ HTML ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ
import pandas as pd               # Ù„ØªØµØ¯ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ CSV/Excel
from dateutil import parser as dateparser  # Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„Ù†ØµÙŠØ©
# -----------------------------
import spacy  # ØªØ­Ù„ÙŠÙ„ Ù„ØºÙˆÙŠ Ø£Ù„Ù…Ø§Ù†ÙŠ
nlp = spacy.load("de_core_news_sm")
# -----------------------------
# 2) Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª 
# -----------------------------
CONFIG = {
    "user": {
        # Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„ØªÙŠ Ø³ØªØ¸Ù‡Ø± ÙÙŠ Ø®Ø·Ø§Ø¨ Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…
        "name": "Saddam Wahan",
        "email": "shoyemen2017@gmail.com",
        "phone": "+49 17656774776",
        "profile_summary": (
            "Kurz zu mir: Ich habe Biomedizintechnik studiert, lerne derzeit Python und interessiere mich fÃ¼r KÃ¼nstliche Intelligenz sowie Embedded Systems."

        ),
        # Ù…Ø¯ÙŠÙ†Ø©/Ù…Ø¯Ù† ØªÙØ¶Ù‘Ù„Ù‡Ø§ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ: ØªÙØ³ØªØ®Ø¯Ù… Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù†Ù‚Ø§Ø· Ø¹Ù†Ø¯ Ø§Ù„ØªØ·Ø§Ø¨Ù‚)
        "preferred_cities": ["Dresden", "NÃ¼rnberg", "Erlangen", "Berlin"]
    },
    "feeds": [
        # Ø£Ù…Ø«Ù„Ø© Ù„Ø±ÙˆØ§Ø¨Ø· RSS Ù…Ù† Indeed (ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ ÙˆØ§Ù„Ø¥Ø¶Ø§ÙØ©)
        "https://de.indeed.com/rss?q=Medizintechnik+Praktikum&l=Dresden",
        "https://de.indeed.com/rss?q=Software+Praktikum&l=Dresden",
        "https://www.stepstone.de/rss/stellenangebote?what=Python&where=Deutschland",],

        # ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¶Ø§ÙØ© Ø±ÙˆØ§Ø¨Ø· RSS Ø£Ø®Ø±Ù‰ Ù‡Ù†Ø§
    "keywords": [
        # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© (ÙƒÙ„ ÙƒÙ„Ù…Ø© ØªÙØ·Ø§Ø¨Ù‚ ØªØ±ÙØ¹ Ø§Ù„Ù†Ù‚Ø§Ø·)
        "Medizintechnik", "Biomedical", "Python", "C++", "Embedded",
        "Praktikum", "Werkstudent", "Quality", "Regulatory", "MDR", "FDA",
        "Signalverarbeitung", "Dresden", "NÃ¼rnberg", "Erlangen"
    ],
    # Ø¹Ø¯Ø¯ Ø£ÙØ¶Ù„ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø®Ø·Ø§Ø¨Ø§Øª ØªÙ‚Ø¯ÙŠÙ… Ù„Ù‡Ø§
    "top_n_letters": 3,
    # Ø§Ø³Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©
    "db_path": "jobs.db",
    # Ø§Ø³Ù… Ù…Ù„Ù Ø§Ù„ØªØµØ¯ÙŠØ±
    "csv_path": "jobs_export.csv",
    # Ù…Ù‡Ù„Ø© Ø§Ù„Ø¬Ù„Ø¨ Ù…Ù† Ø§Ù„ÙˆÙŠØ¨ ÙˆØ§Ù„Ø«Ø¨Ø§Øª (Ø«ÙˆØ§Ù†ÙŠ)
    "http_timeout": 15,
    "sleep_between_requests": 0.8,
    # User-Agent Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø­Ø¸Ø± Ù…Ù† Ø¨Ø¹Ø¶ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹
    "user_agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
}

# --------------------------------------
# 3) Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©: Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª SQLite
# --------------------------------------
def init_db(db_path: str) -> None:
    """Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù† Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹."""
    conn = sqlite3.connect(db_path)
    try:
        cur = conn.cursor()
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS jobs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT,
                company TEXT,
                location TEXT,
                link TEXT UNIQUE,
                published TEXT,
                summary TEXT,
                score REAL,
                source TEXT,
                created_at TEXT
            );
            """
        )
        conn.commit()
    finally:
        conn.close()


def upsert_job(db_path: str, job: Dict[str, Any]) -> None:
    """Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„ÙˆØ¸ÙŠÙØ© ÙÙŠ Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© (ØªØ¬Ø§Ù‡Ù„ Ø¥Ø°Ø§ Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ù†ÙØ³ Ø§Ù„Ø±Ø§Ø¨Ø·)."""
    conn = sqlite3.connect(db_path)
    try:
        cur = conn.cursor()
        cur.execute(
            """
            INSERT OR IGNORE INTO jobs
            (title, company, location, link, published, summary, score, source, created_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?);
            """,
            (
                job.get("title"),
                job.get("company"),
                job.get("location"),
                job.get("link"),
                job.get("published"),
                job.get("summary"),
                job.get("score", 0.0),
                job.get("source"),
                datetime.utcnow().isoformat(timespec="seconds"),
            ),
        )
        conn.commit()
    finally:
        conn.close()


def fetch_all_jobs(db_path: str) -> List[Dict[str, Any]]:
    """Ù‚Ø±Ø§Ø¡Ø© ÙƒÙ„ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ù…Ù† Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ù…Ø±ØªÙ‘Ø¨Ø© Ø­Ø³Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· (score) ØªÙ†Ø§Ø²Ù„ÙŠØ§Ù‹."""
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    try:
        cur = conn.cursor()
        cur.execute("SELECT * FROM jobs ORDER BY score DESC, published DESC;")
        rows = cur.fetchall()
        return [dict(row) for row in rows]
    finally:
        conn.close()

# ------------------------------
# 4) Ø¯ÙˆØ§Ù„: Ø¬Ù„Ø¨ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ù„Ø§ØµØ§Øª
# ------------------------------
def fetch_rss_entries(feed_url: str) -> List[Dict[str, Any]]:
    """Ù‚Ø±Ø§Ø¡Ø© Ø®Ù„Ø§ØµØ§Øª RSS ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ù‚Ø§Ø¦Ù…Ø© Ø¥Ø¯Ø®Ø§Ù„Ø§Øª (entries)."""
    parsed = feedparser.parse(feed_url)
    entries = []
    for e in parsed.entries:
        entry = {
            "title": e.get("title", "").strip(),
            "link": e.get("link", "").strip(),
            "summary": (e.get("summary", "") or e.get("description", "") or "").strip(),
            "published": extract_published(e),
            "source": feed_url,
        }
        entries.append(entry)
    return entries


def extract_published(entry: Any) -> str:
    """Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø± Ù…Ù† Ø¹Ù†Ø§ØµØ± RSS Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ù„ØµÙŠØºØ© ISO."""
    # Ù†Ø­Ø§ÙˆÙ„ Ø¹Ø¯Ø© Ø­Ù‚ÙˆÙ„ Ù…Ø­ØªÙ…Ù„Ø©
    for key in ("published", "updated", "created"):
        val = entry.get(key)
        if val:
            try:
                dt = dateparser.parse(val)
                return dt.isoformat(timespec="seconds")
            except Exception:
                pass
    # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ ØªØ§Ø±ÙŠØ®Ø§Ù‹ ØµØ§Ù„Ø­Ø§Ù‹ØŒ Ù†Ø³ØªØ®Ø¯Ù… ØªØ§Ø±ÙŠØ® Ø§Ù„ÙŠÙˆÙ…
    return datetime.utcnow().isoformat(timespec="seconds")


def fetch_page_text(url: str, timeout: int, ua: str) -> str:
    """Ø¬Ù„Ø¨ ØµÙØ­Ø© Ø§Ù„ÙˆÙŠØ¨ ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ØµÙ‡Ø§ (Ø¨Ø¯ÙˆÙ† ÙˆØ³ÙˆÙ… HTML)."""
    try:
        headers = {"User-Agent": ua}
        resp = requests.get(url, timeout=timeout, headers=headers)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø³ÙƒØ±Ø¨ØªØ§Øª ÙˆØ§Ù„Ø³ØªØ§ÙŠÙ„Ø§Øª
        for bad in soup(["script", "style", "noscript"]):
            bad.decompose()
        text = soup.get_text(separator=" ", strip=True)
        return text
    except Exception:
        return ""
    # ---------------------------------
# ðŸ” NEU: Websuche (deutschlandweite Jobs Ã¼ber Google)
# ---------------------------------
def search_jobs_online(query: str, max_results: int = 15) -> list:
    """
    Sucht Jobs direkt Ã¼ber Google (nicht nur RSS).
    Gibt eine Liste von URLs zurÃ¼ck, die wahrscheinlich Jobanzeigen enthalten.
    """
    from urllib.parse import quote
    url = f"https://www.google.com/search?q={quote(query)}+site:indeed.com+OR+site:stepstone.de+OR+site:adzuna.de+OR+site:workwise.io+OR+site:kimeta.de"
    headers = {"User-Agent": "Mozilla/5.0"}
    resp = requests.get(url, headers=headers)
    soup = BeautifulSoup(resp.text, "html.parser")

    links = []
    for a in soup.select("a"):
        href = a.get("href", "")
        if href.startswith("/url?q="):
            clean = href.split("/url?q=")[1].split("&")[0]
            if "http" in clean and "google" not in clean:
                links.append(clean)
    return links[:max_results]


# ---------------------------------
# 5) Ø¯ÙˆØ§Ù„: Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Ù‚Ø§Ø·
# ---------------------------------
def safe_lower(s: str) -> str:
    """Wandelt den Text sicher in Kleinbuchstaben um."""
    try:
        return s.lower()
    except Exception:
        return s


def contains_negation(text: str) -> bool:
    """
    PrÃ¼ft, ob der Text eine Verneinung enthÃ¤lt (z. B. kein, keine, keinen, nicht).
    Gibt True zurÃ¼ck, wenn eine Verneinung erkannt wird.
    """
    doc = nlp(text)
    for token in doc:
        # dep_ == "neg" bedeutet, das Wort ist eine Verneinung im Satz
        if token.dep_ == "neg" or token.text.lower() in ["kein", "keine", "keinen", "nicht"]:
            return True
    return False


def count_keyword_hits(text: str, keywords: List[str]) -> int:
    """Ø¹Ø¯Ù‘ Ù…Ø±Ø§Øª Ø¸Ù‡ÙˆØ± Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ÙÙŠ Ø§Ù„Ù†Øµ (Ø¨Ø´ÙƒÙ„ ØªÙ‚Ø±ÙŠØ¨ÙŠ)."""
    text_low = safe_lower(text)
    score = 0
    for kw in keywords:
        kw_low = kw.lower()
        # Ù†Ø³ØªØ®Ø¯Ù… Ø¨Ø­Ø«Ø§Ù‹ Ø¨Ø³ÙŠØ·Ø§Ù‹ (ÙŠÙ…ÙƒÙ† ØªØ·ÙˆÙŠØ±Ù‡ Ù„Ø§Ø­Ù‚Ø§Ù‹ Ø¥Ù„Ù‰ Regex Ø¨ÙƒÙ„Ù…Ø§Øª ÙƒØ§Ù…Ù„Ø©)
        hits = text_low.count(kw_low)
        score += hits
    return score


def boost_for_city(text: str, preferred_cities: List[str]) -> int:
    """Ø²ÙŠØ§Ø¯Ø© Ù†Ù‚Ø§Ø· Ø¥Ø°Ø§ Ø¸Ù‡Ø± Ø§Ø³Ù… Ù…Ø¯ÙŠÙ†Ø© Ù…ÙØ¶Ù‘Ù„Ø© ÙÙŠ Ø§Ù„Ù†Øµ."""
    text_low = safe_lower(text)
    return sum(text_low.count(city.lower()) for city in preferred_cities)


def heuristic_company_location(title: str, summary: str) -> (str, str):
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´Ø±ÙƒØ© ÙˆØ§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø¨Ø´ÙƒÙ„ ØªÙ‚Ø±ÙŠØ¨ÙŠ Ù…Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ù„Ù…Ù„Ø®Øµ.
    - Ù…Ù„Ø§Ø­Ø¸Ø©: Ù‡Ø°Ø§ Ù…Ø¬Ø±Ø¯ ØªØ®Ù…ÙŠÙ† Ø¨Ø³ÙŠØ·ØŒ Ù‚Ø¯ Ù„Ø§ ÙŠÙƒÙˆÙ† Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ Ø¯Ø§Ø¦Ù…Ø§Ù‹.
    """
    combined = f"{title} // {summary}"
    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© (ÙƒÙ„Ù…Ø§Øª ØªÙ†ØªÙ‡ÙŠ Ø¨Ù€ GmbH Ù„ÙŠØ³Øª Ù…Ø¯Ù†!)
    loc_match = re.search(r"\b(Berlin|Dresden|MÃ¼nchen|Munich|Hamburg|Erlangen|NÃ¼rnberg|Frankfurt|Stuttgart|Leipzig)\b", combined)
    location = loc_match.group(0) if loc_match else ""

    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´Ø±ÙƒØ© (ÙƒÙ„Ù…Ø§Øª Ù‚Ø¨Ù„ GmbH/AG/SE ÙˆÙ…Ø§ Ø´Ø§Ø¨Ù‡)
    comp_match = re.search(r"\b([A-Z][A-Za-z0-9&.\- ]{1,40})\s+(GmbH|AG|SE|KG|GmbH & Co\. KG)", combined)
    company = comp_match.group(0) if comp_match else ""

    return company, location


def score_job_entry(entry: Dict[str, Any], cfg: Dict[str, Any]) -> Dict[str, Any]:
    """Berechnet die Punktzahl einer Stelle und bereitet die Felder (Firma/Stadt/Zusammenfassung) vor."""
    
    title = entry.get("title", "")
    summary = entry.get("summary", "")
    link = entry.get("link", "")
    published = entry.get("published", "")
    source = entry.get("source", "")

    # Versucht, den vollstÃ¤ndigen Text der Webseite zu laden, um die Genauigkeit zu erhÃ¶hen
    page_text = fetch_page_text(link, cfg["http_timeout"], cfg["user_agent"])
    combined_text = " ".join([title, summary, page_text])

    # ðŸ” Neue Funktion: prÃ¼ft, ob der Text eine Verneinung enthÃ¤lt (z. B. kein, nicht, ...)
    if contains_negation(combined_text):
        print(f"â© Anzeige Ã¼bersprungen (enthÃ¤lt Verneinung): {title}")
        return {
            "title": title,
            "company": "",
            "location": "",
            "link": link,
            "published": published,
            "summary": summary,
            "score": 0.0,   # keine Punkte, weil irrelevant
            "source": source,
        }

    # Wenn keine Verneinung vorhanden ist â†’ Punkte normal berechnen
    base_score = count_keyword_hits(combined_text, cfg["keywords"])
    city_boost = boost_for_city(combined_text, cfg["user"]["preferred_cities"])
    final_score = base_score + city_boost

    company, location = heuristic_company_location(title, summary)

    # KÃ¼rzere Zusammenfassung
    short_summary = (summary[:280] + "â€¦") if len(summary) > 300 else summary

    return {
        "title": title,
        "company": company,
        "location": location,
        "link": link,
        "published": published,
        "summary": short_summary,
        "score": float(final_score),
        "source": source,
    }

# ----------------------------------------
# 6) Ø§Ù„ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰ CSV ÙˆØªÙˆÙ„ÙŠØ¯ Ø®Ø·Ø§Ø¨Ø§Øª Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…
# ----------------------------------------
def export_to_csv(db_path: str, csv_path: str) -> None:
    """ØªØµØ¯ÙŠØ± ÙƒÙ„ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø¥Ù„Ù‰ Ù…Ù„Ù CSV Ù…Ø±ØªÙ‘Ø¨Ø© Ø­Ø³Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø·."""
    rows = fetch_all_jobs(db_path)
    if not rows:
        print("Ù„Ø§ ØªÙˆØ¬Ø¯ ÙˆØ¸Ø§Ø¦Ù Ù„Ø­ÙØ¸Ù‡Ø§ Ø¨Ø¹Ø¯.")
        return
    df = pd.DataFrame(rows)
    df.to_csv(csv_path, index=False, quoting=csv.QUOTE_ALL)
    print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ù: {csv_path}")


def generate_cover_letter(job: Dict[str, Any], user: Dict[str, Any]) -> str:
    """ØªÙˆÙ„ÙŠØ¯ Ø®Ø·Ø§Ø¨ ØªÙ‚Ø¯ÙŠÙ… (Ø¨Ø§Ù„Ø£Ù„Ù…Ø§Ù†ÙŠØ©) Ù…Ø¨Ù†ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø§Ù„Ø¨ ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©."""
    company = job.get("company") or "Ihr Unternehmen"
    job_title = job.get("title") or "Ihre Stelle"
    city = job.get("location") or "Ihre Stadt"
    summary = job.get("summary") or ""

    letter = f"""Sehr geehrte Damen und Herren,

auf die Stelle als *{job_title}* bei *{company}* in *{city}* bin ich mit groÃŸem Interesse gestoÃŸen.
Kurz zu mir: {user.get('profile_summary','')}

Aus der Stellenbeschreibung (Auszug):
{summary}

Ich bin Ã¼berzeugt, dass mein Profil und meine Motivation gut zu den Anforderungen passen.
Gerne Ã¼berzeuge ich Sie in einem persÃ¶nlichen GesprÃ¤ch.

Mit freundlichen GrÃ¼ÃŸen,
{user.get('name','')}
{user.get('email','')} | {user.get('phone','')}
"""
    return letter


def generate_top_letters(db_path: str, user: Dict[str, Any], top_n: int = 3) -> List[str]:
    """ØªÙˆÙ„ÙŠØ¯ Ø®Ø·Ø§Ø¨Ø§Øª ØªÙ‚Ø¯ÙŠÙ… Ù„Ø£Ø¹Ù„Ù‰ N ÙˆØ¸Ø§Ø¦Ù Ø­Ø³Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø·ØŒ ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù†ØµÙˆØµ."""
    rows = fetch_all_jobs(db_path)[:top_n]
    letters = []
    for idx, job in enumerate(rows, 1):
        letter = generate_cover_letter(job, user)
        # Ø­ÙØ¸ ÙƒÙ„ Ø®Ø·Ø§Ø¨ ÙÙŠ Ù…Ù„Ù Ù†ØµÙ‘ÙŠ Ù…Ù†ÙØµÙ„
        fname = f"anschreiben_{idx}.txt"
        with open(fname, "w", encoding="utf-8") as f:
            f.write(letter)
        print(f"âœ‰ï¸  ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø®Ø·Ø§Ø¨: {fname}")
        letters.append(letter)
    if not letters:
        print("Ù„Ø§ ØªÙˆØ¬Ø¯ ÙˆØ¸Ø§Ø¦Ù ÙƒØ§ÙÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø®Ø·Ø§Ø¨Ø§Øª ØªÙ‚Ø¯ÙŠÙ… Ø¨Ø¹Ø¯.")
    return letters

# ------------------------------
# ------------------------------
# 7) Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø®Ø·ÙˆØ§Øª
# ------------------------------
def main():
    # Datenbank vorbereiten
    init_db(CONFIG["db_path"])

    # -------------------------------
    # 1ï¸âƒ£ RSS-Feeds (wie bisher)
    # -------------------------------
    total_found = 0
    for feed in CONFIG["feeds"]:
        print(f"ðŸ“¥ Lese RSS-Feed: {feed}")
        entries = fetch_rss_entries(feed)
        print(f"  â†³ {len(entries)} EintrÃ¤ge gefunden.")

        for e in entries:
            job_row = score_job_entry(e, CONFIG)
            upsert_job(CONFIG["db_path"], job_row)
            total_found += 1
            time.sleep(CONFIG["sleep_between_requests"])

    # -------------------------------
    # 2ï¸âƒ£ Neue Online-Suche (Google)
    # -------------------------------
    print("\nðŸŒ Starte erweiterte Internetsuche ...")
    keywords_to_search = [
        "Medizintechnik Praktikum Deutschland",
        "Werkstudent Biomedical Engineering",
        "Embedded Systems Engineer",
        "Python Developer MedTech",
    ]

    for query in keywords_to_search:
        print(f"ðŸ”Ž Suche: {query}")
        links = search_jobs_online(query)
        print(f"  â†³ {len(links)} Ergebnisse gefunden.")

        for link in links:
            page_text = fetch_page_text(link, CONFIG["http_timeout"], CONFIG["user_agent"])
            if not page_text:
                continue

            entry = {
                "title": query,
                "link": link,
                "summary": page_text[:500],
                "published": datetime.utcnow().isoformat(timespec="seconds"),
                "source": "Google Search",
            }

            job_row = score_job_entry(entry, CONFIG)
            upsert_job(CONFIG["db_path"], job_row)
            total_found += 1
            time.sleep(CONFIG["sleep_between_requests"])

    # -------------------------------
    # 3ï¸âƒ£ Ergebnisse anzeigen und speichern
    # -------------------------------
    print(f"\nâœ… Gesamtanzahl verarbeiteter Anzeigen: {total_found}")

    rows = fetch_all_jobs(CONFIG["db_path"])[:10]
    if not rows:
        print("âš ï¸ Keine passenden Jobs gefunden.")
    else:
        print("\nðŸ† Top 10 Ergebnisse:")
        for i, r in enumerate(rows, 1):
            print(f"{i:02d}. [{r.get('score',0):>3}] {r.get('title','')} â€” {r.get('company','')} â€” {r.get('location','')}")
            print(f"     {r.get('link','')}")

    # Export
    export_to_csv(CONFIG["db_path"], CONFIG["csv_path"])
    generate_top_letters(CONFIG["db_path"], CONFIG["user"], CONFIG["top_n_letters"])

    print("\nâœ… JobFinder-Skript abgeschlossen.")


# ------------------------------
# 8) Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„ (Programmstart)
# ------------------------------
if __name__ == "__main__":
    print("ðŸš€ JobFinder wurde gestartet!")
    main()

